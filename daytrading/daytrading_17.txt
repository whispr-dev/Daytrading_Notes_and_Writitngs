Dynamic and Adaptive Lag Detection for Enhanced Time Series Forecasting
shashank Jain
AI Mind
shashank Jain

·
Follow

Published in
AI Mind

·
9 min read
·
Sep 8, 2024
80






Introduction:

In time series forecasting, one of the most critical yet often overlooked aspects is the determination of the optimal lag, or lookback period. Traditional approaches have relied on static, predetermined lag values, which can significantly limit the model’s ability to capture complex temporal dependencies. This blog post introduces a novel, dynamic approach to lag detection that not only adapts to the data but also integrates seamlessly into the prediction process.

Our method leverages the power of neural networks to dynamically detect and adjust the lag for each prediction, creating a more flexible and robust forecasting system. By parameterizing lag detection and incorporating it directly into the training process, we create a model that can adapt to changing patterns in the data, potentially leading to more accurate predictions.

The key innovations of our approach are:

1. Dynamic Lag Detection: Instead of using a fixed lag, we employ a neural network to determine the optimal lag for each prediction.

2. Adaptive Training: The lag detection is not a separate preprocessing step but an integral part of the model, allowing it to adapt during training.

3. Sliding Window Implementation: We use a sliding window approach, enabling the model to consider recent context when determining the lag.

4. LSTM-based Prediction: The dynamically determined lag is then used to train an LSTM model for the actual prediction task.

In the following sections, we’ll dive deep into each aspect of this approach, exploring the architecture, implementation details, and the potential benefits it offers for time series forecasting.

Problem: The Limitations of Static Lag Detection

Before we delve into our solution, it’s crucial to understand why traditional, static lag detection methods fall short in many scenarios:

1. Inability to Capture Changing Patterns: Time series data often exhibit evolving patterns. A lag that works well for one part of the data might be suboptimal for another.

2. Oversimplification of Complex Dependencies: Real-world time series often have complex temporal dependencies that can’t be captured by a single, fixed lag value.

3. Lack of Adaptability: As new data comes in, the optimal lag might change. Static methods can’t adapt to these changes without manual intervention.

4. One-Size-Fits-All Approach: Different features or aspects of the data might require different lag values. A single, global lag value might not be suitable for all dimensions of the problem.

5. Computational Inefficiency: Methods like grid search to find the optimal lag can be computationally expensive and still result in a static value.

Parameterizing Lag Detection via a Neural Network

At the heart of our approach is the LagLengthNetwork, a neural network designed to dynamically determine the optimal lag for each prediction. This network takes a window of recent data points as input and outputs a suggested lag length. Here’s how we structure this network:

1. LSTM Layer: We start with an LSTM (Long Short-Term Memory) layer. LSTMs are particularly well-suited for processing sequential data, making them ideal for capturing temporal patterns in our input window.

2. Feedforward Layers: The output from the LSTM is then passed through one or more fully connected (feedforward) layers. These layers help in interpreting the patterns detected by the LSTM and mapping them to a lag value.

3. Activation Function: The final layer uses a sigmoid activation function. This constrains the output to a value between 0 and 1, which we then scale to our maximum allowed lag length.

Here’s a simplified version of our LagLengthNetwork implementation in PyTorch:
class LagLengthNetwork(nn.Module):
def __init__(self, input_size=1, hidden_size=32, max_lag=30):
super(LagLengthNetwork, self).__init__()
self.max_lag = max_lag
self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
self.fc = nn.Sequential(
nn.Linear(hidden_size, hidden_size),
nn.ReLU(),
nn.Linear(hidden_size, 1),
nn.Sigmoid()
)

def forward(self, x):
_, (h_n, _) = self.lstm(x)
lag_prob = self.fc(h_n.squeeze(0))
lag = (lag_prob * self.max_lag).round()
return lag

This network takes a sequence of data points as input and outputs a single value representing the suggested lag length. The output is scaled by `max_lag` and rounded to the nearest integer to give us a usable lag value.

Using it as a Sliding Window

The LagLengthNetwork is designed to work with a sliding window of recent data points. This approach allows the network to consider the most recent context when determining the optimal lag. Here’s how we implement this in our training loop:

# Use a window of data to determine lag length
lag_window = current_data[:10].unsqueeze(0) # Shape: (1, 10, 1)
lag_length = lag_net(lag_window)

# Ensure lag_length is valid
lag_length = torch.clamp(lag_length, min=1, max=len(current_data) — 1).squeeze()

# Convert lag_length to a scalar
avg_lag_length = int(lag_length.item())

In this code snippet, we’re using the first 10 data points of our current batch to determine the lag length. This window slides along with our training process, always considering the most recent data points.

The `clamp` function ensures that our lag length stays within reasonable bounds, and we convert the result to an integer for use in creating our training data.

LSTM over that Window: Train for that Lag and Predict

Once we have our dynamically determined lag length, we use it to create our training data and feed it into our main prediction model, which is another LSTM network. Here’s how this process works:

1. Create Training Data: We use the determined lag length to create our X (input) and y (target) data:

X_train, y_train = create_dataset(current_data.cpu().numpy(), avg_lag_length)

2. Train the Prediction Model: We then use this data to train our StockPricePredictor model:

output = price_net(X_train)
loss = criterion(output.squeeze(), y_train)

3. Backpropagate and Update: The loss is then used to update both the prediction model and the lag detection model:

total_loss.backward()
optimizer.step()

This process allows both networks to learn simultaneously. The LagLengthNetwork learns to suggest better lag lengths, while the StockPricePredictor learns to make better predictions given the suggested lag.

Code Explanation

Let’s dive deeper into the key components of our implementation:

Data Preparation

Before we can train our models, we need to prepare our data. We use the yfinance library to download stock price data and preprocess it:

def get_stock_data(ticker, start_date, end_date):
data = yf.download(ticker, start=start_date, end=end_date)[‘Close’]
return data.dropna()

def prepare_data(data):
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data.values.reshape(-1, 1))
train_size = int(len(data_scaled) * 0.8)
train_data = data_scaled[:train_size]
test_data = data_scaled[train_size:]
return train_data, test_data, scaler

These functions download the stock data, scale it using MinMaxScaler (which is crucial for neural network training), and split it into training and testing sets.

StockPricePredictor Model

In addition to our LagLengthNetwork, we have a StockPricePredictor model that does the actual forecasting:

class StockPricePredictor(nn.Module):
def __init__(self, input_size=1, hidden_size=50):
super(StockPricePredictor, self).__init__()
self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
self.fc_out = nn.Linear(hidden_size, 1)

def forward(self, x):
_, (h_n, _) = self.lstm(x)
output = self.fc_out(h_n.squeeze(0))
return output

This model uses an LSTM layer followed by a fully connected layer to make predictions based on the input sequence.

Training Process

The training process is where everything comes together. Let’s break down the `train_model` function:

def train_model(train_data, lag_net, price_net, optimizer, criterion, epochs=100, batch_size=32):
for epoch in range(epochs):
lag_net.train()
price_net.train()

train_losses = []
lag_lengths = []

for i in range(0, len(train_data) — batch_size, batch_size):
batch = train_data[i:i + batch_size]
current_data = torch.tensor(batch, dtype=torch.float32).to(device)

# Use a window of data to determine lag length
lag_window = current_data[:10].unsqueeze(0) # Shape: (1, 10, 1)
lag_length = lag_net(lag_window)

# Ensure lag_length is valid
lag_length = torch.clamp(lag_length, min=1, max=len(current_data) — 1).squeeze()

# Convert lag_length to a scalar
avg_lag_length = int(lag_length.item())
lag_lengths.append(avg_lag_length)

# Use the learned lag length to create the training data
X_train, y_train = create_dataset(current_data.cpu().numpy(), avg_lag_length)

# Convert to tensors
X_train = torch.tensor(X_train, dtype=torch.float32).to(device)
y_train = torch.tensor(y_train, dtype=torch.float32).to(device)

# Forward pass
optimizer.zero_grad()
output = price_net(X_train)
loss = criterion(output.squeeze(), y_train)

# Regularization to encourage diverse lag lengths
lag_diversity_loss = -torch.std(torch.tensor(lag_lengths).float()) + 1e-8
total_loss = loss + 0.1 * lag_diversity_loss

total_loss.backward()
optimizer.step()

train_losses.append(loss.item())

print(f’Epoch {epoch+1}/{epochs}, Loss: {np.mean(train_losses):.4f}, Avg Lag: {np.mean(lag_lengths):.2f}’)

This function does several important things:

1. It iterates over the data in batches.
2. For each batch, it uses the LagLengthNetwork to determine the optimal lag.
3. It creates training data using this lag.
4. It trains the StockPricePredictor on this data.
5. It includes a regularization term to encourage diverse lag lengths.

The regularization term is particularly interesting. By penalizing the model for always choosing the same lag length, we encourage it to be more flexible and adaptive.

Evaluation

After training, we evaluate our model on the test data:

def evaluate_model(test_data, lag_net, price_net, scaler):
lag_net.eval()
price_net.eval()
test_data_tensor = torch.tensor(test_data, dtype=torch.float32).to(device)

with torch.no_grad():
lag_window = test_data_tensor[:10].unsqueeze(0) # Shape: (1, 10, 1)
learned_lag_length = lag_net(lag_window)
learned_lag_length = int(torch.clamp(learned_lag_length, min=1, max=len(test_data) — 1).item())

X_test, y_test = create_dataset(test_data, learned_lag_length)

X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)

with torch.no_grad():
predictions = price_net(X_test_tensor).cpu().numpy()

mae = mean_absolute_error(y_test, predictions)
mape = mean_absolute_percentage_error(y_test, predictions)

print(f’MAE: {mae:.4f}, MAPE: {mape:.4f}’)
print(f’Learned Lag Length: {learned_lag_length:.2f}’)

Results and Analysis

Reliance

MAE: 0.0623, MAPE: 0.0747 Learned Lag Length: 14.00


SBI
MAE: 0.0413, MAPE: 0.0465 Learned Lag Length: 15.00


Several key points stand out:

1. Dynamic Lag Length: The average lag length changes over the course of training, starting around 15 and settling near 19. This demonstrates that our model is indeed learning and adjusting the lag dynamically.

2. Decreasing Loss: The loss decreases over time, indicating that our model is learning to make better predictions.

3. Final Performance: The Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) give us a quantitative measure of our model’s performance. In this case, an MAPE of 2.83% suggests reasonably accurate predictions.

4. Learned Lag Length: The final learned lag length of 19 suggests that, for this particular stock and time period, looking back at the previous 19 days provides the best balance of relevant information for prediction.

Visualizing the Results

To get a better understanding of our model’s performance, we can plot the actual vs. predicted stock prices:

plt.figure(figsize=(10, 6))
plt.plot(scaler.inverse_transform(y_test.reshape(-1, 1)), label=’Actual’)
plt.plot(scaler.inverse_transform(predictions.reshape(-1, 1)), label=’Predicted’)
plt.legend()
plt.title(‘Reliance Stock Price Prediction’)
plt.show()

This visualization allows us to see how closely our predictions track the actual stock prices. It can also help us identify any systematic errors or biases in our predictions.

Potential Improvements

While our approach shows promise, there are several areas where it could be improved:

1. Incorporating More Features: Currently, we’re only using the closing price. Including other features like trading volume, broader market indices, or even sentiment analysis from news sources could potentially improve our predictions.

2. Handling Multiple Stocks: Our current implementation trains separate models for each stock. A more advanced approach could learn to predict multiple stocks simultaneously, potentially uncovering inter-stock relationships.

3. Attention Mechanisms: Incorporating attention mechanisms into our LSTM could allow the model to focus on the most relevant parts of the input sequence, potentially leading to better predictions.

4. Ensemble Methods: Combining predictions from multiple models (including some with fixed lag lengths) could lead to more robust predictions.

5. Hyperparameter Tuning: While we’ve set some hyperparameters (like the maximum lag length and the strength of the lag diversity regularization), these could potentially be optimized using techniques like grid search or Bayesian optimization.

6. Long-term Dependencies: Our current model might struggle with very long-term dependencies. Techniques like dilated RNNs or transformers could potentially capture longer-term patterns more effectively.

Here is the colab link for experimenting
https://colab.research.google.com/drive/177tT0lBdt0EOwuv-PlC2XS4u8rGBVNYb?usp=sharing

Conclusions

Our dynamic lag detection approach represents a step forward in time series forecasting, particularly for financial data. By allowing the model to adaptively determine the most appropriate lag for each prediction, we create a more flexible system that can potentially capture complex temporal dependencies more effectively than traditional fixed-lag approaches.

The key innovations of this approach are:

1. Integration of lag detection into the learning process, allowing it to adapt over time.
2. Use of a sliding window to provide recent context for lag determination.
3. Implementation of a lag diversity loss to encourage exploration of different lag lengths.

While our results are promising, it’s important to note that stock price prediction is an inherently challenging task influenced by numerous factors beyond historical prices. Our model provides a useful tool for analysis, but should not be considered a guaranteed predictor of future stock movements.

Future work could focus on the improvements mentioned above, as well as on applying this technique to other types of time series data beyond stock prices. The principle of dynamic lag detection could potentially be valuable in fields ranging from weather forecasting to energy demand prediction.

In conclusion, this approach demonstrates the potential of integrating traditionally fixed parameters into the learning process of neural networks. By doing so, we create more adaptive and potentially more accurate models for time series forecasting.