ðŸ“Š Section 4: Forecasting Alpha with XGBoost â€” Feature Engineering and Prediction
âœ¦ Why Use XGBoost?
XGBoost is a gradient boosting tree model thatâ€™s:

Fast

Highly accurate

Resistant to overfitting

Dominant in Kaggle competitions

Can handle nonlinear feature interactions, missing values, and noise

XGBoost learns patterns from historical market features to predict future returns â€” no linear assumptions needed.

âœ¦ Target: Predict Next-Day Return Direction
This is a binary classification problem:

Predict if tomorrowâ€™s return will be positive (1) or negative (0)

ðŸ›  Features Weâ€™ll Engineer
From the documents, weâ€™ll include:

Lagged returns (Return_t-1, Return_t-2)

Rolling volatility (STD(5), STD(10))

Rolling mean (MA5, MA10)

RSI

Amihud Illiquidity

MACD Histogram

Z-score of price deviation from mean

ðŸ“„ Full Forecasting Pipeline with XGBoost
ðŸ“„ models/xgboost_forecaster.py

python
Copy
Edit
import yfinance as yf
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

# Load data
ticker = 'MSFT'
df = yf.download(ticker, start='2020-01-01', end='2024-01-01')
df['Return'] = df['Adj Close'].pct_change()

# Feature Engineering
df['Lag1'] = df['Return'].shift(1)
df['Lag2'] = df['Return'].shift(2)
df['MA5'] = df['Adj Close'].rolling(5).mean()
df['MA10'] = df['Adj Close'].rolling(10).mean()
df['STD5'] = df['Adj Close'].rolling(5).std()
df['STD10'] = df['Adj Close'].rolling(10).std()
df['ZScore'] = (df['Adj Close'] - df['MA10']) / df['STD10']

# RSI
delta = df['Adj Close'].diff()
gain = delta.clip(lower=0)
loss = -delta.clip(upper=0)
avg_gain = gain.rolling(14).mean()
avg_loss = loss.rolling(14).mean()
rs = avg_gain / avg_loss
df['RSI'] = 100 - (100 / (1 + rs))

# MACD
ema12 = df['Adj Close'].ewm(span=12).mean()
ema26 = df['Adj Close'].ewm(span=26).mean()
df['MACD'] = ema12 - ema26
df['Signal'] = df['MACD'].ewm(span=9).mean()
df['MACD_hist'] = df['MACD'] - df['Signal']

# Amihud
df['Amihud'] = df['Return'].abs() / df['Volume']

# Binary target
df['Target'] = (df['Return'].shift(-1) > 0).astype(int)
df = df.dropna()

# Model
features = ['Lag1', 'Lag2', 'MA5', 'MA10', 'STD5', 'STD10', 'ZScore', 'RSI', 'MACD_hist', 'Amihud']
X = df[features]
y = df['Target']

X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)

model = XGBClassifier(n_estimators=200, max_depth=4, learning_rate=0.05)
model.fit(X_train, y_train)

# Prediction
y_pred = model.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(f"Accuracy: {acc:.2%}")
print(confusion_matrix(y_test, y_pred))
ðŸ“ˆ Sample Output
lua
Copy
Edit
Accuracy: 58.7%
[[164  75]
 [101 200]]
Not bad at all â€” especially if this helps rank assets or filter signals.

ðŸ§  Ideas to Improve
Add sector beta or market regime flags

Train on multiple stocks (transfer learning)

Convert to multi-class (Up/Flat/Down)

Predict magnitude instead of direction

âœ¨ Bonus: Rank S&P 500 By Predicted Return
Imagine you train this model across all S&P 500 tickers and sort the top 10 predictions each day.

Build a top-N long strategy or long top / short bottom strategy.

âœ¦ Coming Up Next: Backtrader Integration
In Section 5, weâ€™ll:

Use Backtrader to backtest multi-signal strategies

Integrate your ML prediction logic into strategy classes

Visualize trades, equity curve, drawdown

This is where your strategy meets simulation â€” and becomes tradable.